---
title: "class08: PCA Mini Project"
author: "Saba Heydari Seradj (PID: A17002175"
format: pdf
editor: visual
---

It is important to consider scaling your data before PCA.

For example:

```{r}
head(mtcars)
```

```{r}
colMeans(mtcars)
```

```{r}
apply(mtcars,2,sd)
```

```{r}
x <- scale(mtcars)
head(x)
```

```{r}
round(colMeans(x),2)
```

```{r}
round(apply(x,2,sd),2)
```

The mean has been scaled to 0, and the SD to 1.

# Unsupervised Learning Analysis of Human Breast Cancer Cells

## 1) Exploratory Data Analysis

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)

head(wisc.df)
```

We don't want to pass the 'diagnosis' to the PCA, that is just the expert answer that we will later compare our analysis results to. So, we will remove it and also make a vector called 'diagnosis'.

```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
```

```{r}
# Create diagnosis vector for later 
diagnosis <- wisc.df$diagnosis
```

> -   **Q1**. How many observations are in this dataset?

```{r Q1}
nrow(wisc.df)
```

There are 569 observations in the dataset.

> -   **Q2**. How many of the observations have a malignant diagnosis?

```{r Q2}
# Use table to see how many observations have malignant diagnosis
table(diagnosis)['M']
```

There are 212 malignant diagnoses.

**Q3**. How many variables/features in the data are suffixed withÂ `_mean`?

```{r Q3}
mean_names <- grep("_mean$", colnames(wisc.df))
length(mean_names)
```

10 columns have `_mean` in their name.

## PCA

```{r}
# Check column means and standard deviations
round(colMeans(wisc.data),2)
```

```{r}
round(apply(wisc.data,2,sd),2)
```

```{r }
# Perform PCA on wisc.data 
wisc.pr <- prcomp(wisc.data, scale = TRUE)
# Look at summary of results
summary(wisc.pr)
```

> **Q4**. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

0.4427 of the original variance is captured by PC1.

> **Q5**. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

Looking at the cumulative proportion, we can see that 3 PCs are requird to describe at least 70% of the original variance. 

> **Q6.** How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

Looking at the cumulative proportion, we can see that 7 PCs are requird to describe at least 70% of the original variance.

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

It is very messy and difficult to understand. 

```{r}
biplot(wisc.pr)
```


Let's look at what is in this `wisc.pr`
```{r}
attributes(wisc.pr)
```

Main 'PC score plot', 'PC1 vs PC2 plot'

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,2],col=as.factor(diagnosis),xlab = "PC1", ylab = "PC2")
# or plot(wisc.pr$x,col=as.factor(diagnosis))
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
It is kind of similar to PC1 vs PC2 plot, but there is more overlap. 

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,3],col=as.factor(diagnosis),xlab = "PC1", ylab = "PC3")

```

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

It is -0.2608538.

```{r}
head(wisc.pr$rotation)
wisc.pr$rotation["concave.points_mean", 1]
```

> (Q10). What is the minimum number of principal components required to explain 80% of the variance of the data?

We need 5 PCs to explain 80% of the data. 

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

```{r}
plot(wisc.hclust)
abline(h = 19.5, col = "red", lty = 2)
```

> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

It is around 19-20. 

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

> Q11. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
wisc.hclust.clusters2 <- cutree(wisc.hclust, k =2)
table(wisc.hclust.clusters, diagnosis)
```
Two clusters would not work. 

```{r}
wisc.hclust.clusters3 <- cutree(wisc.hclust, k =3)
table(wisc.hclust.clusters, diagnosis)
```
Three clusters is also not going to work. It seems 4 is the smallest number of clusters we can get here that would actually represent the seperation.

> Q12. Which method gives your favorite results for the same data.dist dataset? 

Looking at single, complete, average and ward.D2 methods, my favorite is ward.D2. It is the cleanest-looking tree with good seperation and long branch heights. Ward.D2 method minimizes variance within clusters. 

```{r}
d <- dist(wisc.pr$x[,1:3])
hc <- hclust(d, method='ward.D2')
plot(hc)
```
```{r}
hc2 <- hclust(d, method='single')
plot(hc2)
```
```{r}
hc3 <- hclust(d, method='complete')
plot(hc3)
```
```{r}
hc4 <- hclust(d, method='average')
plot(hc4)
```

```{r}
grps <-  cutree(hc,k=2)
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x, col=grps)
```

```{r}
table(diagnosis, grps)
```
> Q13. How well does the newly created model separate out the two diagnoses? 

The model using PCA with two clusters looks pretty good and as we can see from the table results it has done quite a good job in correctly clustering the datapoints. 

> Q14. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses

As we can see, the model without PCA does a poor job at separating the diagnoses and requires 4 cluster minimum to yield any meaningful results like below. The output after PCA however looks much better and can separate samples well with two clusters. 

```{r}
table(wisc.hclust.clusters, diagnosis)
table(diagnosis, grps)
```
